{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc5e729-9116-4ec9-bf1e-8346cbccdf7b",
   "metadata": {},
   "source": [
    "## Run Clay v1\n",
    "\n",
    "An exercise to do a small but complete analysis from scratch. For this we will\n",
    "\n",
    "1. Set a location and date range of interest\n",
    "2. Download Sentinel-2 imagery for this specification\n",
    "3. Load the model checkpoint\n",
    "4. Prepare data into a format for the model\n",
    "5. Run the model on the imagery\n",
    "6. Analyise the model output (embeddings) using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350e991e-3e73-48f6-b006-b6879458e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17b8a8-a9c6-4053-833e-de97287fae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pystac_client\n",
    "import rioxarray  # noqa: F401\n",
    "import stackstac\n",
    "import torch\n",
    "import yaml\n",
    "from box import Box\n",
    "from matplotlib import pyplot as plt\n",
    "from rasterio.enums import Resampling\n",
    "from shapely import Point\n",
    "from sklearn import decomposition\n",
    "from stacchip.processors.prechip import normalize_timestamp\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from src.model_clay_v1 import ClayMAEModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac6394-9762-422b-9f5d-82d226018c0c",
   "metadata": {},
   "source": [
    "### Specify location and date of interest\n",
    "In this example we will use a location in Portugal where a forest fire happened. We will run the model over the time period of the fire and analyse the model embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7787d-1506-4de7-89dc-c1054910acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point over Monchique Portugal\n",
    "lat, lon = 37.30939, -8.57207\n",
    "\n",
    "# Dates of a large forest fire\n",
    "start = \"2018-07-01\"\n",
    "end = \"2018-09-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd226c9-003b-4867-a64a-8ae887e7e20a",
   "metadata": {},
   "source": [
    "### Get data from STAC catalog\n",
    "\n",
    "Based on the location and date we can obtain a stack of imagery using stackstac. Let's start with finding the STAC items we want to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e80743c-7c77-459b-9984-f6c26cdff549",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAC_API = \"https://earth-search.aws.element84.com/v1\"\n",
    "COLLECTION = \"sentinel-2-l2a\"\n",
    "\n",
    "# Search the catalogue\n",
    "catalog = pystac_client.Client.open(STAC_API)\n",
    "search = catalog.search(\n",
    "    collections=[COLLECTION],\n",
    "    datetime=f\"{start}/{end}\",\n",
    "    bbox=(lon - 1e-5, lat - 1e-5, lon + 1e-5, lat + 1e-5),\n",
    "    max_items=100,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 80}},\n",
    ")\n",
    "\n",
    "all_items = search.get_all_items()\n",
    "\n",
    "# Reduce to one per date (there might be some duplicates\n",
    "# based on the location)\n",
    "items = []\n",
    "dates = []\n",
    "for item in all_items:\n",
    "    if item.datetime.date() not in dates:\n",
    "        items.append(item)\n",
    "        dates.append(item.datetime.date())\n",
    "\n",
    "print(f\"Found {len(items)} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7c68ae-7c8a-446a-8bc7-5afba70183c2",
   "metadata": {},
   "source": [
    "### Create a bounding box around the point of interest\n",
    "\n",
    "This is needed in the projection of the data so that we can generate image chips of the right size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3573b5-5a00-47d9-a648-5c4d7cd2c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinate system from first item\n",
    "epsg = items[0].properties[\"proj:epsg\"]\n",
    "\n",
    "# Convert point of interest into the image projection\n",
    "# (assumes all images are in the same projection)\n",
    "poidf = gpd.GeoDataFrame(\n",
    "    pd.DataFrame(),\n",
    "    crs=\"EPSG:4326\",\n",
    "    geometry=[Point(lon, lat)],\n",
    ").to_crs(epsg)\n",
    "\n",
    "coords = poidf.iloc[0].geometry.coords[0]\n",
    "\n",
    "# Create bounds in projection\n",
    "size = 256\n",
    "gsd = 10\n",
    "bounds = (\n",
    "    coords[0] - (size * gsd) // 2,\n",
    "    coords[1] - (size * gsd) // 2,\n",
    "    coords[0] + (size * gsd) // 2,\n",
    "    coords[1] + (size * gsd) // 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbd3f67-5f2c-46dc-9ee1-2ef1f50fa032",
   "metadata": {},
   "source": [
    "### Retrieve the imagery data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d3824-e48c-4f9d-9c7b-181c0800f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the pixel values, for the bounding box in\n",
    "# the target projection. In this example we use only\n",
    "# the RGB and NIR bands.\n",
    "stack = stackstac.stack(\n",
    "    items,\n",
    "    bounds=bounds,\n",
    "    snap_bounds=False,\n",
    "    epsg=epsg,\n",
    "    resolution=gsd,\n",
    "    dtype=\"float32\",\n",
    "    rescale=False,\n",
    "    fill_value=0,\n",
    "    assets=[\"blue\", \"green\", \"red\", \"nir\"],\n",
    "    resampling=Resampling.nearest,\n",
    ")\n",
    "\n",
    "print(f\"Working with stack of size {stack.shape}\")\n",
    "\n",
    "stack = stack.compute()\n",
    "\n",
    "stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77354bee-bea3-43e0-8936-5808b352e25f",
   "metadata": {},
   "source": [
    "### Let's have a look at the imagery we just downloaded\n",
    "\n",
    "The imagery will contain 7 dates before the fire, of which two are pretty cloudy images. There are also 5 images after the forest fire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468ab57-b44e-4099-9f95-794c803ccc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack.sel(band=[\"red\", \"green\", \"blue\"]).plot.imshow(\n",
    "    row=\"time\", rgb=\"band\", vmin=0, vmax=2000, col_wrap=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa15e9-0285-4cac-816d-a58d2ceda389",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "\n",
    "We now have the data to analyse, let's load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb9b6f-00e4-45a1-b575-2484b5afd511",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "ckpt = \"https://huggingface.co/made-with-clay/Clay/blob/main/Clay-1.0.5.7_epoch-13_val-loss-0.3098.ckpt\"\n",
    "\n",
    "torch.set_default_device(device)\n",
    "\n",
    "model = ClayMAEModule.load_from_checkpoint(\n",
    "    ckpt, metadata_path=\"../configs/metadata.yaml\", shuffle=False, mask_ratio=0\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9422d855-c73b-43c3-9a06-fd89d7f4eb08",
   "metadata": {},
   "source": [
    "### Prepare band metadata for passing it to the model\n",
    "\n",
    "This is the most technical part so far. We will take the information in the stack of imagery and convert it into the formate that the model requires. This includes converting the lat/lon and the date of the imagery into normalized values.\n",
    "\n",
    "The Clay model will accept any band combination in any order, from different platforms. But for this the model needs to know the wavelength of each band that is passed to it, and normalization parameters for each band as well. It will use that to normalize the data and to interpret each band based on its central wavelength.\n",
    "\n",
    "For Sentinel-2 we can use medata file of the model to extract those values. But this cloud also be something custom for a different platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9cb385-1aa6-453f-9a5b-1b388a95c407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mean, std, and wavelengths from metadata\n",
    "platform = \"sentinel-2-l2a\"\n",
    "metadata = Box(yaml.safe_load(open(\"../configs/metadata.yaml\")))\n",
    "mean = []\n",
    "std = []\n",
    "waves = []\n",
    "# Use the band names to get the correct values in the correct order.\n",
    "for band in stack.band:\n",
    "    mean.append(metadata[platform].bands.mean[str(band.values)])\n",
    "    std.append(metadata[platform].bands.std[str(band.values)])\n",
    "    waves.append(metadata[platform].bands.wavelength[str(band.values)])\n",
    "\n",
    "# Prepare the normalization transform function using the mean and std values.\n",
    "transform = v2.Compose(\n",
    "    [\n",
    "        v2.Normalize(mean=mean, std=std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d8e1f3-011d-4be5-8071-547f0ad91ad6",
   "metadata": {},
   "source": [
    "### Convert the band pixel data in to the format for the model\n",
    "\n",
    "We will take the information in the stack of imagery and convert it into the formate that the model requires. This includes converting the lat/lon and the date of the imagery into normalized values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bbe0c2-2cc3-428c-8d38-7e9516b6134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep datetimes embedding using a normalization function from the model code.\n",
    "datetimes = stack.time.values.astype(\"datetime64[s]\").tolist()\n",
    "times = [normalize_timestamp(dat) for dat in datetimes]\n",
    "week_norm = [dat[0] for dat in times]\n",
    "hour_norm = [dat[1] for dat in times]\n",
    "\n",
    "\n",
    "# Prep lat/lon embedding using the\n",
    "def normalize_latlon(lat, lon):\n",
    "    lat = lat * np.pi / 180\n",
    "    lon = lon * np.pi / 180\n",
    "\n",
    "    return (math.sin(lat), math.cos(lat)), (math.sin(lon), math.cos(lon))\n",
    "\n",
    "\n",
    "latlons = [normalize_latlon(lat, lon)] * len(times)\n",
    "lat_norm = [dat[0] for dat in latlons]\n",
    "lon_norm = [dat[1] for dat in latlons]\n",
    "\n",
    "# Normalize pixels\n",
    "pixels = torch.from_numpy(stack.data.astype(np.float32))\n",
    "pixels = transform(pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe34640d-48df-41b0-a749-4d6ada1a42d7",
   "metadata": {},
   "source": [
    "### Combine the metadata and the transformed pixels\n",
    "\n",
    "Now we can combine all of these inputs into a dictionary that combines everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fea0d4-f3ac-4430-a7d0-7b3c302a0754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare additional information\n",
    "datacube = {\n",
    "    \"platform\": platform,\n",
    "    \"time\": torch.tensor(\n",
    "        np.hstack((week_norm, hour_norm)),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    ),\n",
    "    \"latlon\": torch.tensor(\n",
    "        np.hstack((lat_norm, lon_norm)), dtype=torch.float32, device=device\n",
    "    ),\n",
    "    \"pixels\": pixels.to(device),\n",
    "    \"gsd\": torch.tensor(stack.gsd.values, device=device),\n",
    "    \"waves\": torch.tensor(waves, device=device),\n",
    "}\n",
    "datacube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a18e52b-b21e-4c58-a1f2-26b66d73ecbe",
   "metadata": {},
   "source": [
    "### Run the model\n",
    "\n",
    "Pass the datacube we prepared to the model to create embeddings. This will create one embedding vector for each of the images we downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69335005-ebd1-4edf-b493-5e314637ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    unmsk_patch, unmsk_idx, msk_idx, msk_matrix = model.model.encoder(datacube)\n",
    "\n",
    "# The first embedding is the class token, which is the\n",
    "# overall single embedding. We extract that for PCA below.\n",
    "embeddings = unmsk_patch[:, 0, :].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3ebb8-18a2-4918-b863-01ea36095d9f",
   "metadata": {},
   "source": [
    "### Analyse the embeddings\n",
    "\n",
    "A simple analysis of the embeddings is to reduce each one of them into a single number using Principal Component Analysis. For this we will fit a PCA on the 12 embeddings we have, and do the dimensionality reduction for them. We will se a separation into three groups, the previous images, the cloudy images, and the images after the fire, they all fall into a different range of the PCA space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8debf91-e38a-46d2-81c9-24b71a3adfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA\n",
    "pca = decomposition.PCA(n_components=1)\n",
    "pca_result = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.xticks(rotation=-45)\n",
    "\n",
    "# Plot all points in blue first\n",
    "plt.scatter(stack.time, pca_result, color=\"blue\")\n",
    "\n",
    "# Re-plot cloudy images in green\n",
    "plt.scatter(stack.time[0], pca_result[0], color=\"green\")\n",
    "plt.scatter(stack.time[2], pca_result[2], color=\"green\")\n",
    "\n",
    "# Color all images after fire in red\n",
    "plt.scatter(stack.time[-5:], pca_result[-5:], color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b3877-2fa5-4d68-a77d-50e4f01b29ef",
   "metadata": {},
   "source": [
    "### Exercise: change location of interest\n",
    "\n",
    "We invite you to think of a place and date that you would like to analyse like this and try it out for yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claymodel",
   "language": "python",
   "name": "claymodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
