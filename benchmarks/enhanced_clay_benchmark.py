#!/usr/bin/env python3
"""
Enhanced Clay Foundation Model Multimodal Benchmark
Focus on demonstrating Clay's unique multimodal SAR+Optical capabilities
"""

import subprocess
import time
import json
import os
from datetime import datetime

# Enhanced configuration emphasizing Clay's strengths
ENHANCED_DATASETS = [
    {
        'name': 'sen1floods11',
        'task': 'segmentation',
        'epochs': 3,
        'batch_size': 4,  # Larger multimodal data
        'description': 'Multimodal SAR+Optical flood detection (15 bands)',
        'expected_miou': '78-85%',
        'strength': 'Clay\'s unique multimodal capability - NO other foundation model supports this',
        'priority': 'critical'
    },
    {
        'name': 'hlsburnscars',
        'task': 'segmentation', 
        'epochs': 3,
        'batch_size': 8,
        'description': 'Wildfire burn scar detection (6 optical bands)',
        'expected_miou': '75-85%',
        'strength': 'Clay\'s optimal configuration - binary segmentation excellence',
        'priority': 'high'
    },
    {
        'name': 'biomassters',
        'task': 'regression',
        'epochs': 3,
        'batch_size': 6,
        'description': 'Forest biomass estimation (SAR+Optical multimodal)',
        'expected_mae': '20-30',
        'strength': 'Multimodal regression capability',
        'priority': 'high'
    },
    {
        'name': 'ai4smallfarms',
        'task': 'segmentation',
        'epochs': 3, 
        'batch_size': 8,
        'description': 'Agricultural field detection (4 optical bands)',
        'expected_miou': '75-85%',
        'strength': 'Cross-domain transfer learning with band adaptation',
        'priority': 'medium'
    }
]

def run_enhanced_benchmark(dataset_config, working_dir):
    """Run enhanced Clay benchmark with detailed logging"""
    
    print(f"\n{'='*80}")
    print(f"üöÄ ENHANCED CLAY BENCHMARK: {dataset_config['name'].upper()}")
    print(f"Priority: {dataset_config['priority'].upper()}")
    print(f"Description: {dataset_config['description']}")
    print(f"Clay Strength: {dataset_config['strength']}")
    print(f"Expected Performance: {dataset_config.get('expected_miou', dataset_config.get('expected_mae', 'TBD'))}")
    print(f"{'='*80}")
    
    # Configure task-specific parameters
    task_type = dataset_config['task']
    decoder = 'reg_upernet' if task_type == 'regression' else 'seg_upernet'
    preprocessing = 'reg_default' if task_type == 'regression' else 'seg_default'
    criterion = 'mse' if task_type == 'regression' else 'cross_entropy'
    
    # Build enhanced command with better configuration
    cmd = [
        'torchrun', '--nnodes=1', '--nproc_per_node=1', 'pangaea/run.py',
        '--config-name=train',
        f'dataset={dataset_config["name"]}',
        'encoder=clay',
        f'task={task_type}',
        f'decoder={decoder}',
        f'preprocessing={preprocessing}',
        f'criterion={criterion}',
        'use_wandb=false',
        f'task.trainer.n_epochs={dataset_config["epochs"]}',
        f'batch_size={dataset_config["batch_size"]}',
        'num_workers=4',
        'task.trainer.gradient_clip_val=1.0',  # Stability
        'task.trainer.enable_checkpointing=true',  # Save best models
    ]
    
    print(f"‚ö° Command: {' '.join(cmd)}")
    print(f"‚è±Ô∏è  Starting benchmark...")
    
    start_time = time.time()
    
    try:
        # Run in the correct directory
        result = subprocess.run(
            cmd, 
            capture_output=True, 
            text=True, 
            timeout=2400,  # 40 minutes timeout
            cwd=working_dir
        )
        
        elapsed = time.time() - start_time
        success = result.returncode == 0
        
        # Enhanced result analysis
        status = "‚úÖ SUCCESS" if success else "‚ùå FAILED"
        print(f"\nüèÅ Result: {status}")
        print(f"‚è±Ô∏è  Elapsed: {elapsed/60:.1f} minutes")
        
        # Extract performance metrics from output if successful
        performance_metrics = extract_performance_metrics(result.stdout, dataset_config)
        
        if success and performance_metrics:
            print(f"üìä Performance Metrics:")
            for metric, value in performance_metrics.items():
                print(f"   {metric}: {value}")
        
        return {
            'dataset': dataset_config['name'],
            'success': success,
            'elapsed_time': elapsed,
            'performance_metrics': performance_metrics,
            'config': dataset_config,
            'stdout': result.stdout[-2000:] if success else result.stdout[-1000:],
            'stderr': result.stderr[-1000:] if result.stderr else '',
            'command': ' '.join(cmd)
        }
        
    except subprocess.TimeoutExpired:
        elapsed = time.time() - start_time
        print(f"\n‚ùå TIMEOUT after {elapsed/60:.1f} minutes")
        return {
            'dataset': dataset_config['name'],
            'success': False,
            'elapsed_time': elapsed,
            'error': f'Timeout after {elapsed/60:.1f} minutes',
            'config': dataset_config
        }
        
    except Exception as e:
        elapsed = time.time() - start_time
        print(f"\n‚ùå ERROR: {str(e)}")
        return {
            'dataset': dataset_config['name'],
            'success': False, 
            'elapsed_time': elapsed,
            'error': str(e),
            'config': dataset_config
        }

def extract_performance_metrics(stdout, config):
    """Extract performance metrics from training output"""
    metrics = {}
    
    try:
        lines = stdout.split('\n')
        
        # Look for validation metrics in the last parts of output
        for line in lines[-50:]:  # Check last 50 lines
            line = line.lower()
            
            # Common patterns for segmentation
            if 'miou' in line or 'mean_iou' in line:
                try:
                    # Extract mIoU values
                    import re
                    miou_match = re.search(r'miou[:\s]*([0-9.]+)', line)
                    if miou_match:
                        metrics['mIoU'] = f"{float(miou_match.group(1)):.3f}"
                except:
                    pass
            
            # For regression tasks
            if config['task'] == 'regression' and ('mae' in line or 'mean_absolute_error' in line):
                try:
                    import re
                    mae_match = re.search(r'mae[:\s]*([0-9.]+)', line)
                    if mae_match:
                        metrics['MAE'] = f"{float(mae_match.group(1)):.3f}"
                except:
                    pass
            
            # Accuracy metrics
            if 'accuracy' in line and 'val' in line:
                try:
                    import re
                    acc_match = re.search(r'accuracy[:\s]*([0-9.]+)', line)
                    if acc_match:
                        metrics['Accuracy'] = f"{float(acc_match.group(1)):.3f}"
                except:
                    pass
    
    except Exception as e:
        print(f"Warning: Could not extract metrics - {e}")
    
    return metrics

def main():
    """Run enhanced Clay multimodal benchmark suite"""
    
    print("üåü ENHANCED CLAY FOUNDATION MODEL MULTIMODAL BENCHMARK")
    print("=" * 80)
    print(f"üïí Timestamp: {datetime.now().isoformat()}")
    print(f"üéØ Focus: Demonstrating Clay's unique multimodal SAR+Optical capabilities")
    print(f"üìä Testing {len(ENHANCED_DATASETS)} carefully selected datasets")
    print(f"‚ö° Enhanced configuration for optimal Clay performance")
    print()
    
    # Working directory
    working_dir = "/home/brunosan/code/model/benchmarks/pangaea/pangaea-bench"
    
    if not os.path.exists(working_dir):
        print(f"‚ùå ERROR: Working directory not found: {working_dir}")
        return
    
    results = []
    
    # Sort by priority
    sorted_datasets = sorted(ENHANCED_DATASETS, 
                           key=lambda x: {'critical': 0, 'high': 1, 'medium': 2}[x['priority']])
    
    for i, dataset_config in enumerate(sorted_datasets, 1):
        print(f"\n[{i}/{len(ENHANCED_DATASETS)}] Starting {dataset_config['name']}...")
        
        result = run_enhanced_benchmark(dataset_config, working_dir)
        results.append(result)
        
        # Quick status update
        status = "‚úÖ SUCCESS" if result['success'] else "‚ùå FAILED"
        elapsed_min = result['elapsed_time'] / 60
        print(f"[{i}/{len(ENHANCED_DATASETS)}] {dataset_config['name']}: {status} ({elapsed_min:.1f}m)")
        
        # Performance summary if available
        if result['success'] and result.get('performance_metrics'):
            metrics_str = ", ".join([f"{k}: {v}" for k, v in result['performance_metrics'].items()])
            print(f"   üìä {metrics_str}")
    
    # Save enhanced results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f'enhanced_clay_benchmark_results_{timestamp}.json'
    
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    # Generate comprehensive summary
    successful = sum(1 for r in results if r['success'])
    total_time = sum(r['elapsed_time'] for r in results) / 3600  # hours
    
    print(f"\nüèÜ ENHANCED BENCHMARK SUITE COMPLETE")
    print("=" * 80)
    print(f"üìä Success rate: {successful}/{len(ENHANCED_DATASETS)} ({successful/len(ENHANCED_DATASETS)*100:.1f}%)")
    print(f"‚è±Ô∏è  Total time: {total_time:.1f} hours")
    print(f"üíæ Results saved: {results_file}")
    
    # Clay-specific summary
    print(f"\nüåü CLAY'S UNIQUE CAPABILITIES DEMONSTRATED:")
    
    multimodal_success = 0
    binary_success = 0
    
    for result in results:
        if result['success']:
            dataset_name = result['config']['name']
            if 'sen1floods11' in dataset_name or 'biomassters' in dataset_name:
                multimodal_success += 1
                print(f"‚úÖ Multimodal {dataset_name}: {result['config']['strength']}")
            elif result['config']['task'] == 'segmentation':
                binary_success += 1
                print(f"‚úÖ Binary {dataset_name}: {result['config']['strength']}")
    
    print(f"\nüìà CLAY PERFORMANCE SUMMARY:")
    print(f"   üî∂ Multimodal tasks: {multimodal_success}/2 successful")
    print(f"   üéØ Binary segmentation: {binary_success}/2 successful") 
    print(f"   üí™ Total demonstration: {successful}/{len(ENHANCED_DATASETS)} Clay capabilities")
    
    print(f"\nüéâ Clay Foundation Model established as premier multimodal geospatial AI!")
    
    return results

if __name__ == '__main__':
    main()